import json
import os
import sys
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
import torch

# === è¨­å®šï¼ˆä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«æŒ‡å®šï¼‰ ===
MODEL_NAME = "google/gemma-3-1b-it"  # å¿…è¦ãªãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
HF_CACHE_DIR = os.path.expanduser("~/.cache/huggingface/hub")

def list_local_models():
    """ Hugging Face ã®ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾— """
    if not os.path.exists(HF_CACHE_DIR):
        return {"error": "Hugging Face ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"}

    models = []
    for entry in os.listdir(HF_CACHE_DIR):
        if entry.startswith("models--"):
            model_name = entry.replace("models--", "").replace("--", "/")
            models.append(model_name)

    return {"local_models": models} if models else {"error": "ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ Hugging Face ãƒ¢ãƒ‡ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"}

# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã®é–‹å§‹
# print(f"ğŸ”¹ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰: {MODEL_NAME}", file=sys.stderr)

try:
    # è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ `vocab_size` ãŒã‚ã‚‹ã‹ç¢ºèª
    config = AutoConfig.from_pretrained(MODEL_NAME)
    if not hasattr(config, "vocab_size"):
        raise ValueError("ãƒ¢ãƒ‡ãƒ«è¨­å®šã« vocab_size ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map="auto")

except Exception as e:
    # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ JSON å½¢å¼ã§å‡ºåŠ›
    error_message = {
        "error": f"ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {MODEL_NAME}",
        "reason": str(e),
        "solution": [
            "Hugging Face ã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã€é©åˆ‡ãªã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚",
            "ãƒ¢ãƒ‡ãƒ«ã®åˆ©ç”¨è¦ç´„ï¼ˆã‚¢ã‚°ãƒªãƒ¡ãƒ³ãƒˆï¼‰ã«åŒæ„ã—ã¦ãã ã•ã„ã€‚",
            "ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
            f"  huggingface-cli login --token YOUR_HF_TOKEN",
            f"  huggingface-cli download {MODEL_NAME}"
        ],
        "huggingface_model_url": f"https://huggingface.co/{MODEL_NAME}",
        "local_models": list_local_models()
    }
    print(json.dumps(error_message, ensure_ascii=False, indent=4))
    sys.exit(1)

# æ¨™æº–å…¥åŠ›ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
prompt = sys.stdin.read().strip()
if not prompt:
    print(json.dumps({"error": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒç©ºã§ã™ã€‚"}))
    sys.exit(1)

# å…¥åŠ›ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

# å¿œç­”ã®ç”Ÿæˆ
output = model.generate(**inputs, max_length=100)
response_text = tokenizer.decode(output[0], skip_special_tokens=True)

# JSON å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
response_json = {
    "model": MODEL_NAME,
    "prompt": prompt,
    "response": response_text
}

# é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›
# print("Gemmaã®å¿œç­”:", response_text)

# JSON å½¢å¼ã§ã®å‡ºåŠ›
print(json.dumps(response_json, ensure_ascii=False, indent=4))
