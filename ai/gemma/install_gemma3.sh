#!/bin/bash

set -e  # ã‚¨ãƒ©ãƒ¼æ™‚ã«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’åœæ­¢

# ======================================================
# === è¨­å®šï¼ˆå¤‰æ›´å¯èƒ½ãªå¤‰æ•°ï¼‰
# ======================================================

# ä»®æƒ³ç’°å¢ƒã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
PYTHON_ENV_DIR="$HOME/venvs/gemma3"

# Hugging Face API ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
HF_HOME="$HOME/.cache/huggingface"
HF_TOKEN="YOUR_HF_TOKEN_HERE"  # å¿…ãšã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š

# ======================================================
# Google Gemma 3ã‚·ãƒªãƒ¼ã‚ºå¯¾å¿œGPU & å¿…è¦VRAMä¸€è¦§
# ======================================================
declare -A GEMMA_MODELS
GEMMA_MODELS=(
    ["google/gemma-3-1b-it"]=4
    ["google/gemma-3-4b-it"]=8
    ["google/gemma-3-12b-it"]=16
    ["google/gemma-3-27b-it"]=48
)

# === VRAM è¦ä»¶ãƒã‚§ãƒƒã‚¯ & é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«é¸æŠ ===
echo "ğŸ”¹ GPU ã® VRAM å®¹é‡ã‚’å–å¾—ä¸­..."
AVAILABLE_VRAM_GB=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | awk '{print $1/1024}' | sort -nr | head -n1)

echo "âœ… ä½¿ç”¨å¯èƒ½ãª VRAM: ${AVAILABLE_VRAM_GB}GB"

# ä½¿ç”¨å¯èƒ½ãª VRAM ã«å¿œã˜ã¦å¯¾å¿œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—
echo "ğŸ”¹ ä½¿ç”¨å¯èƒ½ãª Gemma ãƒ¢ãƒ‡ãƒ«ä¸€è¦§:"
AVAILABLE_MODELS=()
for MODEL in "${!GEMMA_MODELS[@]}"; do
    if (( $(echo "$AVAILABLE_VRAM_GB >= ${GEMMA_MODELS[$MODEL]}" | bc -l) )); then
        AVAILABLE_MODELS+=("$MODEL")
        echo "   - $MODEL (æ¨å¥¨ VRAM: ${GEMMA_MODELS[$MODEL]}GB)"
    fi
done

# æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•é¸æŠï¼ˆæœ€å¤§ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼‰
MODEL_NAME="${AVAILABLE_MODELS[-1]}"
REQUIRED_VRAM_GB="${GEMMA_MODELS[$MODEL_NAME]}"

echo "âœ… è‡ªå‹•é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«: $MODEL_NAME (æ¨å®šå¿…è¦VRAM: ${REQUIRED_VRAM_GB}GB)"

# === CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®å–å¾— ===
get_cuda_version() {
    echo "ğŸ”¹ `nvcc` ã§ CUDA ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å–å¾—..."
    if command -v nvcc &>/dev/null; then
        CUDA_VERSION=$(nvcc --version | grep "release" | awk '{print $NF}' | sed -E 's/[V,]//g' | cut -d'.' -f1,2)
        CUDA_VERSION="cu$(echo $CUDA_VERSION | tr -d '.')"
        echo "âœ… æ­£ã—ãæ¤œå‡ºã•ã‚ŒãŸ CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³: $CUDA_VERSION"
    else
        echo "âŒ `nvcc` ã‚³ãƒãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼CUDA ãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        exit 1
    fi
}

echo "ğŸ”¹ Google Gemma 3 ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’é–‹å§‹ã—ã¾ã™..."

# 1. å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
echo "ğŸ”¹ å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­..."
sudo apt update && sudo apt upgrade -y
sudo apt install -y python3 python3-venv python3-pip git curl bc xdg-utils

# 2. CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®å–å¾—
get_cuda_version

# 3. Hugging Face ã«ãƒ­ã‚°ã‚¤ãƒ³ï¼ˆAPIã‚­ãƒ¼ãŒå¿…è¦ï¼‰
echo "ğŸ”¹ Hugging Face ã®èªè¨¼ã‚’ç¢ºèªä¸­..."
mkdir -p "$HF_HOME"

if [[ "$HF_TOKEN" == "YOUR_HF_TOKEN_HERE" || -z "$HF_TOKEN" ]]; then
    echo "âš ï¸ Hugging Face ã® API ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    echo "ğŸŒ Hugging Face ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½œæˆãƒšãƒ¼ã‚¸ã‚’é–‹ãã¾ã™..."
    xdg-open "https://huggingface.co/settings/tokens"

    read -p "ğŸ”‘ API ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: " HF_TOKEN
    if [ -z "$HF_TOKEN" ]; then
        echo "âŒ API ãƒˆãƒ¼ã‚¯ãƒ³ãŒå…¥åŠ›ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚"
        exit 1
    fi
fi

echo -n "$HF_TOKEN" > "$HF_HOME/token"
huggingface-cli login --token "$HF_TOKEN"

# 4. ãƒ¢ãƒ‡ãƒ«åˆ©ç”¨è¦ç´„ï¼ˆã‚¢ã‚°ãƒªãƒ¡ãƒ³ãƒˆï¼‰ã®ç¢ºèª
echo "ğŸ”¹ ãƒ¢ãƒ‡ãƒ«ã®åˆ©ç”¨è¦ç´„ã‚’ç¢ºèªä¸­..."
AGREEMENT_CHECK=$(curl -s -H "Authorization: Bearer $HF_TOKEN" "https://huggingface.co/api/models/$MODEL_NAME")
if echo "$AGREEMENT_CHECK" | grep -q "error"; then
    echo "âš ï¸ ãƒ¢ãƒ‡ãƒ«ã®åˆ©ç”¨è¦ç´„ã«åŒæ„ã—ã¦ã„ã¾ã›ã‚“ã€‚"
    echo "ğŸŒ ãƒ¢ãƒ‡ãƒ«ã®åˆ©ç”¨è¨±å¯ãƒšãƒ¼ã‚¸ã‚’é–‹ãã¾ã™..."
    xdg-open "https://huggingface.co/$MODEL_NAME"
    echo "ğŸ”¹ ãƒšãƒ¼ã‚¸ã‚’é–‹ã„ãŸã‚‰ã€ŒAccess Modelã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦æ‰¿è«¾ã—ã¦ãã ã•ã„ã€‚"
    exit 1
fi

# 5. Python ä»®æƒ³ç’°å¢ƒã®ä½œæˆ
PYTHON_ENV_DIR="$HOME/venvs/${MODEL_NAME//\//-}"  # ãƒ¢ãƒ‡ãƒ«åã«å¿œã˜ãŸä»®æƒ³ç’°å¢ƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
if [ ! -d "$PYTHON_ENV_DIR" ]; then
    echo "ğŸ”¹ Python ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆä¸­..."
    python3 -m venv "$PYTHON_ENV_DIR"
else
    echo "âœ… æ—¢ã«ä»®æƒ³ç’°å¢ƒãŒå­˜åœ¨ã—ã¾ã™: $PYTHON_ENV_DIR"
fi

# ä»®æƒ³ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–
source "$PYTHON_ENV_DIR/bin/activate"

# 6. å¿…è¦ãª Python ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
echo "ğŸ”¹ å¿…è¦ãª Python ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­..."
pip install --upgrade pip setuptools wheel
pip install torch torchvision torchaudio --index-url "https://download.pytorch.org/whl/$CUDA_VERSION"
pip install --upgrade git+https://github.com/huggingface/transformers.git
pip install huggingface_hub accelerate

# 7. Hugging Face ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
echo "ğŸ”¹ Google Gemma 3 ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­..."
HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download --token "$HF_TOKEN" $MODEL_NAME

# 8. Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ
echo "ğŸ”¹ Google Gemma 3 ã‚’å®Ÿè¡Œã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ..."
cat <<EOF > run_gemma.py
import json
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ãƒ¢ãƒ‡ãƒ«å
model_name = "$MODEL_NAME"

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")

# å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
prompt = "ã“ã‚“ã«ã¡ã¯ã€è‡ªå·±ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

# å¿œç­”ã®ç”Ÿæˆ
output = model.generate(**inputs, max_length=100)
response_text = tokenizer.decode(output[0], skip_special_tokens=True)

# JSON å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
response_json = {
    "model": model_name,
    "prompt": prompt,
    "response": response_text
}

# é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›
print("Gemmaã®å¿œç­”:", response_text)

# JSON å½¢å¼ã§ã®å‡ºåŠ›
print(json.dumps(response_json, ensure_ascii=False, indent=4))
EOF

# 9. å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
echo "ğŸ”¹ Google Gemma 3 ã®å‹•ä½œç¢ºèªã‚’é–‹å§‹..."
python run_gemma.py

# ä»®æƒ³ç’°å¢ƒã‚’çµ‚äº†
deactivate

echo "âœ… Google Gemma 3 ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸï¼"
echo "ğŸ“Œ ä»®æƒ³ç’°å¢ƒã®å ´æ‰€: $PYTHON_ENV_DIR"
echo "ğŸ”¹ ä»®æƒ³ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–: source $PYTHON_ENV_DIR/bin/activate"
echo "ğŸ”¹ å®Ÿè¡Œ: python run_gemma.py"
echo "ğŸ”¹ çµ‚äº†: deactivate"
