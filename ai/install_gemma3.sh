#!/bin/bash

set -e  # ã‚¨ãƒ©ãƒ¼æ™‚ã«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’åœæ­¢

# === è¨­å®š ===
PYTHON_ENV_DIR="$HOME/venvs/gemma3"
MODEL_NAME="google/gemma-3-12b-it"
CUDA_VERSION=""

# === CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®å–å¾— ===
get_cuda_version() {
    echo "ğŸ”¹ `nvcc` ã§ CUDA ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å–å¾—..."
    if command -v nvcc &>/dev/null; then
        CUDA_VERSION=$(nvcc --version | grep "release" | awk '{print $NF}' | sed -E 's/[V,]//g' | cut -d'.' -f1,2)
        CUDA_VERSION="cu$(echo $CUDA_VERSION | tr -d '.')"
        echo "âœ… æ­£ã—ãæ¤œå‡ºã•ã‚ŒãŸ CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³: $CUDA_VERSION"
    else
        echo "âŒ `nvcc` ã‚³ãƒãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼CUDA ãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        exit 1
    fi
}

echo "ğŸ”¹ Google Gemma 3 ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’é–‹å§‹ã—ã¾ã™..."

# 1. å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
echo "ğŸ”¹ å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­..."
sudo apt update && sudo apt upgrade -y
sudo apt install -y python3 python3-venv python3-pip git curl

# 2. CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®å–å¾—
get_cuda_version

# 3. Python ä»®æƒ³ç’°å¢ƒã®ä½œæˆ
if [ ! -d "$PYTHON_ENV_DIR" ]; then
    echo "ğŸ”¹ Python ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆä¸­..."
    python3 -m venv "$PYTHON_ENV_DIR"
else
    echo "âœ… æ—¢ã«ä»®æƒ³ç’°å¢ƒãŒå­˜åœ¨ã—ã¾ã™: $PYTHON_ENV_DIR"
fi

# ä»®æƒ³ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–
source "$PYTHON_ENV_DIR/bin/activate"

# 4. å¿…è¦ãª Python ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
echo "ğŸ”¹ å¿…è¦ãª Python ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­..."
pip install --upgrade pip setuptools wheel
pip install torch torchvision torchaudio --index-url "https://download.pytorch.org/whl/$CUDA_VERSION"
pip install transformers huggingface_hub accelerate

# 5. Hugging Face ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
echo "ğŸ”¹ Google Gemma 3 ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­..."
pip install "huggingface_hub[hf_transfer]"
HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download $MODEL_NAME

# 6. Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ
echo "ğŸ”¹ Google Gemma 3 ã‚’å®Ÿè¡Œã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ..."
cat <<EOF > run_gemma.py
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "$MODEL_NAME"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")

prompt = "ã“ã‚“ã«ã¡ã¯ã€è‡ªå·±ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

output = model.generate(**inputs, max_length=100)
response = tokenizer.decode(output[0], skip_special_tokens=True)

print("Gemmaã®å¿œç­”:", response)
EOF

# 7. å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
echo "ğŸ”¹ Google Gemma 3 ã®å‹•ä½œç¢ºèªã‚’é–‹å§‹..."
python run_gemma.py

# ä»®æƒ³ç’°å¢ƒã‚’çµ‚äº†
deactivate

echo "âœ… Google Gemma 3 ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸï¼"
echo "ğŸ“Œ ä»®æƒ³ç’°å¢ƒã®å ´æ‰€: $PYTHON_ENV_DIR"
echo "ğŸ”¹ ä»®æƒ³ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–: source $PYTHON_ENV_DIR/bin/activate"
echo "ğŸ”¹ å®Ÿè¡Œ: python run_gemma.py"
echo "ğŸ”¹ çµ‚äº†: deactivate"
