#!/bin/bash

set -e  # ã‚¨ãƒ©ãƒ¼æ™‚ã«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’åœæ­¢

# ======================================================
# Google Gemma 3ã‚·ãƒªãƒ¼ã‚ºå¯¾å¿œGPU & å¿…è¦VRAMä¸€è¦§
# ======================================================
# | ãƒ¢ãƒ‡ãƒ«å               | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | å¿…è¦VRAM (æ¨å®š) | æ¨å¥¨GPU            |
# |------------------------|-------------|----------------|--------------------|
# | google/gemma-3-1b-it   | 1B          | ç´„ 4GB         | RTX 3050 ä»¥ä¸Š      |
# | google/gemma-3-4b-it   | 4B          | ç´„ 8GB         | RTX 3060 ä»¥ä¸Š      |
# | google/gemma-3-12b-it  | 12B         | ç´„ 16GB        | RTX 3090 / A6000  |
# | google/gemma-3-27b-it  | 27B         | ç´„ 48GB        | A100 / H100       |
#
#  ğŸ’¡ VRAMãŒè¶³ã‚Šãªã„å ´åˆã¯ 4bit é‡å­åŒ–(QLoRA)ãªã©ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚
# ======================================================

# === è¨­å®š ===
PYTHON_ENV_DIR="$HOME/venvs/gemma3"
MODEL_NAME="google/gemma-3-12b-it"
CUDA_VERSION=""
REQUIRED_VRAM_GB=16  # é¸æŠãƒ¢ãƒ‡ãƒ«ã®æ¨å®šVRAMè¦ä»¶ï¼ˆfloat16ï¼‰

# === CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®å–å¾— ===
get_cuda_version() {
    echo "ğŸ”¹ `nvcc` ã§ CUDA ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å–å¾—..."
    if command -v nvcc &>/dev/null; then
        CUDA_VERSION=$(nvcc --version | grep "release" | awk '{print $NF}' | sed -E 's/[V,]//g' | cut -d'.' -f1,2)
        CUDA_VERSION="cu$(echo $CUDA_VERSION | tr -d '.')"
        echo "âœ… æ­£ã—ãæ¤œå‡ºã•ã‚ŒãŸ CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³: $CUDA_VERSION"
    else
        echo "âŒ `nvcc` ã‚³ãƒãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼CUDA ãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        exit 1
    fi
}

# === VRAM è¦ä»¶ãƒã‚§ãƒƒã‚¯ ===
check_vram() {
    echo "ğŸ”¹ GPU ã® VRAM å®¹é‡ã‚’å–å¾—ä¸­..."
    AVAILABLE_VRAM_GB=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | awk '{print $1/1024}' | sort -nr | head -n1)
    
    if (( $(echo "$AVAILABLE_VRAM_GB < $REQUIRED_VRAM_GB" | bc -l) )); then
        echo "âš ï¸ è­¦å‘Š: ã“ã®ãƒ¢ãƒ‡ãƒ«ã«ã¯å°‘ãªãã¨ã‚‚ ${REQUIRED_VRAM_GB}GB ã® VRAM ãŒå¿…è¦ã§ã™ãŒã€ç¾åœ¨ã® VRAM ã¯ ${AVAILABLE_VRAM_GB}GB ã§ã™ã€‚"
        echo "ğŸ’¡ ãƒ¢ãƒ‡ãƒ«ã®é‡å­åŒ–ï¼ˆ8bit, 4bitï¼‰ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
    else
        echo "âœ… å¿…è¦ãª VRAM å®¹é‡ãŒç¢ºä¿ã•ã‚Œã¦ã„ã¾ã™ã€‚ï¼ˆ${AVAILABLE_VRAM_GB}GBï¼‰"
    fi
}

echo "ğŸ”¹ Google Gemma 3 ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’é–‹å§‹ã—ã¾ã™..."

# 1. å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
echo "ğŸ”¹ å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­..."
sudo apt update && sudo apt upgrade -y
sudo apt install -y python3 python3-venv python3-pip git curl bc

# 2. CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®å–å¾—
get_cuda_version

# 3. VRAM ã®ç¢ºèª
check_vram

# 4. Python ä»®æƒ³ç’°å¢ƒã®ä½œæˆ
if [ ! -d "$PYTHON_ENV_DIR" ]; then
    echo "ğŸ”¹ Python ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆä¸­..."
    python3 -m venv "$PYTHON_ENV_DIR"
else
    echo "âœ… æ—¢ã«ä»®æƒ³ç’°å¢ƒãŒå­˜åœ¨ã—ã¾ã™: $PYTHON_ENV_DIR"
fi

# ä»®æƒ³ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–
source "$PYTHON_ENV_DIR/bin/activate"

# 5. å¿…è¦ãª Python ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
echo "ğŸ”¹ å¿…è¦ãª Python ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­..."
pip install --upgrade pip setuptools wheel
pip install torch torchvision torchaudio --index-url "https://download.pytorch.org/whl/$CUDA_VERSION"
pip install transformers huggingface_hub accelerate

# 6. Hugging Face ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
echo "ğŸ”¹ Google Gemma 3 ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­..."
pip install "huggingface_hub[hf_transfer]"
HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download $MODEL_NAME

# 7. Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ
echo "ğŸ”¹ Google Gemma 3 ã‚’å®Ÿè¡Œã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ..."
cat <<EOF > run_gemma.py
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "$MODEL_NAME"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")

prompt = "ã“ã‚“ã«ã¡ã¯ã€è‡ªå·±ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

output = model.generate(**inputs, max_length=100)
response = tokenizer.decode(output[0], skip_special_tokens=True)

print("Gemmaã®å¿œç­”:", response)
EOF

# 8. å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
echo "ğŸ”¹ Google Gemma 3 ã®å‹•ä½œç¢ºèªã‚’é–‹å§‹..."
python run_gemma.py

# ä»®æƒ³ç’°å¢ƒã‚’çµ‚äº†
deactivate

echo "âœ… Google Gemma 3 ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸï¼"
echo "ğŸ“Œ ä»®æƒ³ç’°å¢ƒã®å ´æ‰€: $PYTHON_ENV_DIR"
echo "ğŸ”¹ ä»®æƒ³ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–: source $PYTHON_ENV_DIR/bin/activate"
echo "ğŸ”¹ å®Ÿè¡Œ: python run_gemma.py"
echo "ğŸ”¹ çµ‚äº†: deactivate"
