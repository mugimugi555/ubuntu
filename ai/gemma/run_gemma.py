import json
import os
import sys
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# === ãƒ¢ãƒ‡ãƒ«åã‚’è‡ªå‹•è¨­å®š ===
HF_CACHE_DIR = os.path.expanduser("~/.cache/huggingface/hub")
MODEL_CANDIDATES = [
    "google/gemma-3-27b-it",
    "google/gemma-3-12b-it",
    "google/gemma-3-4b-it",
    "google/gemma-3-1b-it"
]

def find_model():
    """ Hugging Face ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’æ¢ã™ """
    for model in MODEL_CANDIDATES:
        model_path = os.path.join(HF_CACHE_DIR, f"models--{model.replace('/', '--')}")
        if os.path.exists(model_path):
            return model
    return None

# ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚Œã°ãã¡ã‚‰ã‚’ä½¿ç”¨
MODEL_NAME = os.getenv("MODEL_NAME", find_model())

if not MODEL_NAME:
    print(json.dumps({"error": "åˆ©ç”¨å¯èƒ½ãª Gemma ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"}))
    sys.exit(1)

# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã®é–‹å§‹
print(f"ğŸ”¹ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰: {MODEL_NAME}", file=sys.stderr)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map="auto")

# æ¨™æº–å…¥åŠ›ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
prompt = sys.stdin.read().strip()
if not prompt:
    print(json.dumps({"error": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒç©ºã§ã™ã€‚"}))
    sys.exit(1)

# å…¥åŠ›ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

# å¿œç­”ã®ç”Ÿæˆ
output = model.generate(**inputs, max_length=100)
response_text = tokenizer.decode(output[0], skip_special_tokens=True)

# JSON å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
response_json = {
    "model": MODEL_NAME,
    "prompt": prompt,
    "response": response_text
}

# é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›
print("Gemmaã®å¿œç­”:", response_text)

# JSON å½¢å¼ã§ã®å‡ºåŠ›
print(json.dumps(response_json, ensure_ascii=False, indent=4))
